# Julius Caesar RAG Expert System

## 1. Project Overview

This project implements a **Retrieval-Augmented Generation (RAG)** expert system for *Shakespeareâ€™s Julius Caesar*.  
It enables users to ask natural-language questions about the play and receive:

- **Scholarly, citation-rich answers**
- **Act/Sceneâ€“grounded evidence**
- **Transparent retrieved chunks**
- **Confidence-weighted retrieval explanations**

### ğŸ”§ End-to-End System Components

- PDF ingestion & structured chunking of *Julius Caesar*
- **Multi-level semantic chunking**:
  - Speaker-level
  - Context-window
  - Scene-level
  - Explanation-level (manual summaries)
- Dense embeddings stored in **ChromaDB** (persistent)
- **Weighted retrieval** across four semantic granularities
- **Gemini 2.0 Flash** for generation
- **FastAPI backend + Streamlit frontend**
  - Users interact via Streamlit
  - Streamlit â†’ FastAPI â†’ RAG pipeline â†’ Answer + Evidence

## Folder Structure
``` 
backend/
â”‚
â”œâ”€â”€ Dockerfile # Backend Docker config
â”œâ”€â”€ main.py # FastAPI backend entrypoint
â”œâ”€â”€ rag.py # Core RAG pipeline
â”œâ”€â”€ requirements.txt # Backend dependencies
â”‚
â”œâ”€â”€ chroma_julius_caesar/ # Persistent ChromaDB (vector store)
â”‚ â””â”€â”€ chroma.sqlite3 # Chroma database file 
â”‚
â”œâ”€â”€ chunking/ # Chunk-generation scripts
â”‚ â”œâ”€â”€ context_window.py
â”‚ â”œâ”€â”€ scene_level.py
â”‚ â”œâ”€â”€ scene_level_summarised.py
â”‚ â””â”€â”€ speaker_level.py
â”‚
â”œâ”€â”€ indexing/ # Index-building scripts
â”‚ â””â”€â”€ generate_chromadb.py
â”‚
â”œâ”€â”€ chunks/ # Pre-generated chunk JSONL files
â”‚ â”œâ”€â”€ julius_caesar_chunks.jsonl
â”‚ â”œâ”€â”€ julius_caesar_context_windows.jsonl
â”‚ â”œâ”€â”€ julius_caesar_explanation_chunks.jsonl
â”‚ â””â”€â”€ julius_caesar_scene_chunks.jsonl
â”‚
â”œâ”€â”€ evaluation/ # RAG evaluation utilities
â”‚ â”œâ”€â”€ evaluate.py
â”‚ â”œâ”€â”€ questions.json
â”‚ â””â”€â”€ rag_dataset.json
â”‚
frontend/
â”‚
â”œâ”€â”€ Dockerfile # Frontend Docker config
â””â”€â”€ frontend.py # Streamlit UI
```
---

## 2. System Architecture
<img width="570" height="813" alt="Screenshot from 2025-11-16 22-34-47" src="https://github.com/user-attachments/assets/1ae4eb14-5cd0-4a70-98cc-8d298e30069c" />


---

## 3. Design Choices

## âœ… Design Justification

This RAG system uses **four granularities of chunking**, each solving a different retrieval failure mode in Shakespearean text.

---

### ### **A. Speaker-Level Chunks**

**Purpose**
- Capture individual speeches and who says what
- Useful for questions about motives, personality, conflict

**Why Needed**
- Shakespeareâ€™s meaning often lies in single lines
- Enables pinpoint retrieval  
  *Example:* â€œWhat does Cassius say about fate?â€

---

### ### **B. Context-Window Chunks**

**Purpose**
- Capture dialogue flow using sliding windows of 3â€“5 speeches

**Why Needed**
- Pure speaker chunks break continuity
- Restores local narrative context
- Helps with interpretation where meaning spans multiple lines

---

### ### **C. Scene-Level Chunks**

**Purpose**
- Capture global thematic, emotional, and narrative meaning

**Why Needed**
- Many questions require entire-scene context:
  - Character motivations  
  - Emotional build-ups  
  - Causal relationships  
- Crucial for interpretive literature analysis

---

### ### **D. Explanation-Level Chunks (Manual Summaries)**

**Purpose**
- Provide human-level, concise summaries of scenes/acts  
- Help answer high-level conceptual or thematic questions

**Why Needed**
- LLMs perform best with a â€œmiddle abstraction layerâ€
- Enables complex reasoning such as:
  *â€œCompare Caesarâ€™s ambition with Brutusâ€™s values.â€*

---

## â­ Why Four Levels?

Each chunk type has strengths and weaknesses:

| Chunk Type      | Strength                  | Weakness                         |
|-----------------|---------------------------|----------------------------------|
| Speaker         | High precision            | No broader context               |
| Window          | Restores dialogue flow    | Still narrow for thematic analysis |
| Scene           | Rich global meaning       | Long for some questions          |
| Explanation     | Strong conceptual clarity | Loses exact text fidelity        |

**Combining all four produces the strongest hybrid retrieval system.**

---

## Embedding Model Choice: **BGE Base (bge-base-en-v1.5)**

### âœ” Why BGE?

- State-of-the-art dense retrieval performance  
- Strong for abstract + literary semantic similarity  
- Outperforms MiniLM, MPNet, GloVe on long textual sequences  
- Free, open-source, reproducible  
- Excellent for Shakespeareâ€™s long, structured lines

### âœ” Why not OpenAI/Gemini embeddings?

- Costly  
- Not offline-friendly  
- Harder to reproduce  
- BGE gives consistent, high-quality performance locally

---

## LLM Choice: **Gemini 2.0 Flash**

### âœ” Why Gemini 2.0 Flash?

- Extremely fast (ideal for interactive systems)
- Strong reasoning on literature content
- Great contextual grounding
- Low-latency generation
- Handles multi-paragraph answers + citations effectively

---

## âœ” System Prompt Enforcement

The LLM is strictly constrained to:

- Use **only retrieved context**
- Avoid hallucinations
- Cite **Act/Scene consistently**
- Maintain a scholarly tone

This ensures answers match academic expectations.

---
## Instructions: Run Project Using Docker Compose

### 1. Requirements
Install:
- Docker
- Docker Compose

### 2. Project Structure
Ensure a `.env` file exists with:
GEMINI_API_KEY=your_api_key_here

### 3. Build the Docker Images
docker compose build

### 4. Start the System
docker compose up

### 5. Access the Application
- Frontend (Streamlit): http://localhost:8501  
- Backend (FastAPI): http://localhost:8000  
- API Docs: http://localhost:8000/docs  

### 6. Stop the Containers

 - docker compose down

---
## Contribution 

## 1. Chunk Strategies, ChromaDB Indexing, Web UI Interface
- Chunking Strategies  
- ChromaDB Indexing  
- Backend
- Frotnend 

## 2. RAGs Evaluation, Retriever, Docker, EVAL.md
- RAGs Evaluation  
- Retriever  
- Docker  
- EVAL.md  

## 3. Test Bed Questions, EVAL.md, Docker
- Test Bed Questions  
- EVAL.md  
- Docker
- Prompting Technique

